{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapes ``https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.*`` in order to get all business objects and their properties.\n",
    "\n",
    "To begin with, the most interesting interfaces (``ITour``, ``IConsignment``, ``ITransportOrder`` and ``ITourStatus``) are scraped. If a property is a complex type (which means another interface, class or enum from us), the scraper will recursively scrape the properties of the complex type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "\n",
    "BASE_URL = \"https://docs.soloplan.de/carlo/api/\"\n",
    "\n",
    "DOCS_TO_SCRAPE = [\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITour.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.IConsignment.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITransportOrder.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITourStatus.html\",\n",
    "]\n",
    "\n",
    "saved_interfaces = set()\n",
    "\n",
    "\n",
    "def extract_docfx_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract the main content article\n",
    "    article = soup.find(\"article\", class_=\"content wrap\")\n",
    "\n",
    "    # Ensure article is found\n",
    "    if not article:\n",
    "        print(f\"Article not found at {url}.\")\n",
    "        return\n",
    "\n",
    "    # Extract interface details\n",
    "    name = article.find(\"h1\").text.strip()\n",
    "    if name.startswith(\"Interface \"):\n",
    "        name = name[10:]\n",
    "        data_type = \"interface\"\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    if name in saved_interfaces:\n",
    "        print(f\"Skipping {name} as it already exists.\")\n",
    "        return\n",
    "    saved_interfaces.add(name)\n",
    "\n",
    "    # search for <div class=\"markdown level0 summary\">, that is next to the h1 and get the summary text\n",
    "    overall_summary = \"\"\n",
    "    summary_tag = article.find_next(\"div\", class_=\"markdown level0 summary\")\n",
    "    if summary_tag:\n",
    "        summary_p = summary_tag.find(\"p\")\n",
    "        if summary_p:\n",
    "            overall_summary = summary_p.text.strip()\n",
    "\n",
    "    if name.startswith(\"Class \"):\n",
    "        name = name[7:]\n",
    "        data_type = \"class\"\n",
    "    if name.startswith(\"Enum \"):\n",
    "        name = name[6:]\n",
    "        data_type = \"enum\"\n",
    "\n",
    "    # Extract Namespace and Assembly\n",
    "    namespace = \"Namespace not found.\"\n",
    "    assembly = \"Assembly not found.\"\n",
    "    for h6 in article.find_all(\"h6\"):\n",
    "        text = h6.text.strip()\n",
    "        if text.startswith(\"Namespace\"):\n",
    "            namespace = text[11:].strip()\n",
    "        elif text.startswith(\"Assembly\"):\n",
    "            assembly = text[10:].strip()\n",
    "\n",
    "    # Extract properties and methods\n",
    "    properties = []\n",
    "    type_references = []\n",
    "    property_headers = article.find_all(\"h4\")\n",
    "    for header in property_headers:\n",
    "        property_name = header.text.strip()\n",
    "        property_summary_tag = header.find_next(\"div\", class_=\"markdown level1 summary\")\n",
    "        if property_summary_tag:\n",
    "            try:\n",
    "                property_summary = property_summary_tag.find(\"p\").text.strip()\n",
    "            except AttributeError:\n",
    "                property_summary = \"Summary not found.\"\n",
    "        else:\n",
    "            property_summary = \"Summary not found.\"\n",
    "\n",
    "        declaration_code_tag = header.find_next(\"div\", class_=\"codewrapper\")\n",
    "        if declaration_code_tag:\n",
    "            declaration_code = declaration_code_tag.find(\"code\").text.strip()\n",
    "        else:\n",
    "            declaration_code = \"Declaration not found.\"\n",
    "\n",
    "        property_value_type_tag = header.find_next(\"h5\", string=\"Property Value\")\n",
    "        if property_value_type_tag:\n",
    "            property_value_type_td = property_value_type_tag.find_next(\"td\")\n",
    "            if property_value_type_td.find(\"a\"):\n",
    "                property_value_type = property_value_type_td.find(\"a\").text.strip()\n",
    "                type_url = BASE_URL + property_value_type_td.find(\"a\")[\"href\"]\n",
    "                # recursively scrape the type if it's not already in the list\n",
    "                if type_url not in DOCS_TO_SCRAPE:\n",
    "                    DOCS_TO_SCRAPE.append(type_url)\n",
    "                    # save the recursive type reference\n",
    "                    type_references.append(property_value_type)\n",
    "            else:\n",
    "                property_value_type = property_value_type_td.text.strip()\n",
    "        else:\n",
    "            property_value_type = \"Property type not found.\"\n",
    "\n",
    "        properties.append(\n",
    "            {\n",
    "                \"name\": property_name,\n",
    "                \"summary\": property_summary,\n",
    "                \"declaration\": declaration_code,\n",
    "                \"type\": property_value_type,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Extract extension methods\n",
    "    extension_methods = []\n",
    "    extension_header = article.find(\"h3\", text=\"Extension Methods\")\n",
    "    if extension_header:\n",
    "        links = extension_header.find_next_siblings(\"div\")\n",
    "        for link in links:\n",
    "            method_name = link.find(\"a\").text.strip()\n",
    "            extension_methods.append(method_name)\n",
    "    else:\n",
    "        extension_methods = [\"Extension methods not found.\"]\n",
    "\n",
    "    return (\n",
    "        name,\n",
    "        {\n",
    "            \"summary\": overall_summary,\n",
    "            \"type\": data_type,\n",
    "            \"namespace\": namespace,\n",
    "            \"assembly\": assembly,\n",
    "            \"properties\": properties,\n",
    "            \"extension_methods\": extension_methods,\n",
    "            \"type_references\": type_references,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "all_interfaces = {}\n",
    "\n",
    "while DOCS_TO_SCRAPE:\n",
    "    url = DOCS_TO_SCRAPE.pop(0)\n",
    "    result = extract_docfx_data(url)\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    name, item = result\n",
    "    if name in all_interfaces:\n",
    "        continue\n",
    "\n",
    "    all_interfaces[name] = item\n",
    "    print(f\"Processed {name}\")\n",
    "\n",
    "# Save all interfaces to a single JSON file\n",
    "with open(\"scraped_domain_knowledge.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    ordered_interfaces = OrderedDict(sorted(all_interfaces.items()))\n",
    "    json.dump(ordered_interfaces, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embeddings, the JSON structure might not be the best choice, thus these files need to be converted to plain text. Additional metdata will be extracted and added to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"documents\"):\n",
    "    os.makedirs(\"documents\")\n",
    "\n",
    "\n",
    "def transform_interface(key, value):\n",
    "    doc = f\"Interface: {key}\\n\"\n",
    "    #doc += f\"Namespace: {interface['namespace']}\\n\"\n",
    "    #doc += f\"Assembly: {interface['assembly']}\\n\\n\"\n",
    "    doc += f\"Summary: {value['summary']}\\n\\n\"\n",
    "    doc += \"Properties:\\n\"\n",
    "\n",
    "    for i, prop in enumerate(value[\"properties\"], 1):\n",
    "        summary = prop['summary'].strip().replace('\\n', ' ').replace('\\r', '')\n",
    "        doc += f\"- {prop['name']}\\n\"\n",
    "        doc += f\"   - Type: {prop['type']}\\n\"\n",
    "        doc += f\"   - Description: {summary}\\n\"\n",
    "        # doc += f\"   - Declaration: {prop['declaration']}\\n\\n\"\n",
    "    \n",
    "    # doc += \"\\n\"\n",
    "\n",
    "    # doc += \"Extension Methods:\\n\"\n",
    "    # for method in value.get(\"extension_methods\", []):\n",
    "    #     doc += f\"- {method}\\n\"\n",
    "\n",
    "    return doc\n",
    "\n",
    "def extract_metadata(key, value):\n",
    "    return {\n",
    "        \"name\": key,\n",
    "        \"summary\": value[\"summary\"],\n",
    "        \"type\": value[\"type\"],\n",
    "        \"namespace\": value[\"namespace\"],\n",
    "        \"assembly\": value[\"assembly\"],\n",
    "        \"type_references\": value[\"type_references\"],\n",
    "    }\n",
    "\n",
    "# Load all interfaces from the single JSON file\n",
    "with open(\"scraped_domain_knowledge.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    interface = json.load(f)\n",
    "\n",
    "# Transform and save the documents\n",
    "for key, value in interface.items():\n",
    "    doc_text = transform_interface(key, value)\n",
    "    with open(f\"documents/{key}.txt\", \"w\") as f:\n",
    "        f.write(doc_text)\n",
    "    metadata = extract_metadata(key, value)\n",
    "    with open(f\"documents/{key}.metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
