{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapes ``https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.*`` in order to get all business objects and their properties.\n",
    "\n",
    "To begin with, the most interesting interfaces (``ITour``, ``IConsignment``, ``ITransportOrder`` and ``ITourStatus``) are scraped. If a property is a complex type (which means another interface, class or enum from us), the scraper will recursively scrape the properties of the complex type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "\n",
    "BASE_URL = \"https://docs.soloplan.de/carlo/api/\"\n",
    "\n",
    "DOCS_TO_SCRAPE = [\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITour.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.IConsignment.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITransportOrder.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITourStatus.html\",\n",
    "]\n",
    "\n",
    "saved_interfaces = set()\n",
    "\n",
    "\n",
    "def extract_docfx_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract the main content article\n",
    "    article = soup.find(\"article\", class_=\"content wrap\")\n",
    "\n",
    "    # Ensure article is found\n",
    "    if not article:\n",
    "        print(f\"Article not found at {url}.\")\n",
    "        return\n",
    "\n",
    "    # Extract interface details\n",
    "    name = article.find(\"h1\").text.strip()\n",
    "    if name.startswith(\"Interface \"):\n",
    "        name = name[10:]\n",
    "        data_type = \"interface\"\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    if name in saved_interfaces:\n",
    "        print(f\"Skipping {name} as it already exists.\")\n",
    "        return\n",
    "    saved_interfaces.add(name)\n",
    "\n",
    "    # search for <div class=\"markdown level0 summary\">, that is next to the h1 and get the summary text\n",
    "    overall_summary = \"\"\n",
    "    summary_tag = article.find_next(\"div\", class_=\"markdown level0 summary\")\n",
    "    if summary_tag:\n",
    "        summary_p = summary_tag.find(\"p\")\n",
    "        if summary_p:\n",
    "            overall_summary = summary_p.text.strip()\n",
    "\n",
    "    if name.startswith(\"Class \"):\n",
    "        name = name[7:]\n",
    "        data_type = \"class\"\n",
    "    if name.startswith(\"Enum \"):\n",
    "        name = name[6:]\n",
    "        data_type = \"enum\"\n",
    "\n",
    "    # Extract Namespace and Assembly\n",
    "    namespace = \"Namespace not found.\"\n",
    "    assembly = \"Assembly not found.\"\n",
    "    for h6 in article.find_all(\"h6\"):\n",
    "        text = h6.text.strip()\n",
    "        if text.startswith(\"Namespace\"):\n",
    "            namespace = text[11:].strip()\n",
    "        elif text.startswith(\"Assembly\"):\n",
    "            assembly = text[10:].strip()\n",
    "\n",
    "    # Extract properties and methods\n",
    "    properties = []\n",
    "    type_references = []\n",
    "    property_headers = article.find_all(\"h4\")\n",
    "    for header in property_headers:\n",
    "        property_name = header.text.strip()\n",
    "        property_summary_tag = header.find_next(\"div\", class_=\"markdown level1 summary\")\n",
    "        if property_summary_tag:\n",
    "            try:\n",
    "                property_summary = property_summary_tag.find(\"p\").text.strip()\n",
    "            except AttributeError:\n",
    "                property_summary = \"Summary not found.\"\n",
    "        else:\n",
    "            property_summary = \"Summary not found.\"\n",
    "\n",
    "        declaration_code_tag = header.find_next(\"div\", class_=\"codewrapper\")\n",
    "        if declaration_code_tag:\n",
    "            declaration_code = declaration_code_tag.find(\"code\").text.strip()\n",
    "        else:\n",
    "            declaration_code = \"Declaration not found.\"\n",
    "\n",
    "        property_value_type_tag = header.find_next(\"h5\", string=\"Property Value\")\n",
    "        if property_value_type_tag:\n",
    "            property_value_type_td = property_value_type_tag.find_next(\"td\")\n",
    "            if property_value_type_td.find(\"a\"):\n",
    "                property_value_type = property_value_type_td.find(\"a\").text.strip()\n",
    "                type_url = BASE_URL + property_value_type_td.find(\"a\")[\"href\"]\n",
    "                # recursively scrape the type if it's not already in the list\n",
    "                if type_url not in DOCS_TO_SCRAPE:\n",
    "                    DOCS_TO_SCRAPE.append(type_url)\n",
    "                    # save the recursive type reference\n",
    "                    type_references.append(property_value_type)\n",
    "            else:\n",
    "                property_value_type = property_value_type_td.text.strip()\n",
    "        else:\n",
    "            property_value_type = \"Property type not found.\"\n",
    "\n",
    "        properties.append(\n",
    "            {\n",
    "                \"name\": property_name,\n",
    "                \"summary\": property_summary,\n",
    "                \"declaration\": declaration_code,\n",
    "                \"type\": property_value_type,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Extract extension methods\n",
    "    extension_methods = []\n",
    "    extension_header = article.find(\"h3\", text=\"Extension Methods\")\n",
    "    if extension_header:\n",
    "        links = extension_header.find_next_siblings(\"div\")\n",
    "        for link in links:\n",
    "            method_name = link.find(\"a\").text.strip()\n",
    "            extension_methods.append(method_name)\n",
    "    else:\n",
    "        extension_methods = [\"Extension methods not found.\"]\n",
    "\n",
    "    return (\n",
    "        name,\n",
    "        {\n",
    "            \"summary\": overall_summary,\n",
    "            \"type\": data_type,\n",
    "            \"namespace\": namespace,\n",
    "            \"assembly\": assembly,\n",
    "            \"properties\": properties,\n",
    "            \"extension_methods\": extension_methods,\n",
    "            \"type_references\": type_references,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "all_interfaces = {}\n",
    "\n",
    "while DOCS_TO_SCRAPE:\n",
    "    url = DOCS_TO_SCRAPE.pop(0)\n",
    "    result = extract_docfx_data(url)\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    name, item = result\n",
    "    if name in all_interfaces:\n",
    "        continue\n",
    "\n",
    "    all_interfaces[name] = item\n",
    "    print(f\"Processed {name}\")\n",
    "\n",
    "# Save all interfaces to a single JSON file\n",
    "with open(\"scraped_domain_knowledge.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    ordered_interfaces = OrderedDict(sorted(all_interfaces.items()))\n",
    "    json.dump(ordered_interfaces, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embeddings, the JSON structure might not be the best choice, thus these files need to be converted to plain text. Additional metdata will be extracted and added to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import tiktoken\n",
    "\n",
    "# Ensure you have the necessary directory structure\n",
    "if not os.path.exists(\"documents\"):\n",
    "    os.makedirs(\"documents\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "max_tokens = 8192\n",
    "\n",
    "\n",
    "def split_text_intelligently(\n",
    "    properties: List[str], header: str, max_tokens=8192, max_bytes=32766\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Split the text intelligently based on the token limit. Since properties of interfaces are mostly used in this dataset, 'intelligently' means that we split the properties in a way that they are not split across multiple chunks.\n",
    "\n",
    "    Args:\n",
    "        properties (List[str]): List of property strings.\n",
    "        header (str): File header string.\n",
    "        max_tokens (int): Maximum number of token that the embedding model can handle.\n",
    "        max_bytes (int): Maximum number of bytes allowed in a chunk (based on Azure AI Search limits).\n",
    "    \"\"\"\n",
    "    header_tokens = tokenizer.encode(header)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk = header\n",
    "    current_tokens = header_tokens.copy()\n",
    "\n",
    "    for prop in properties:\n",
    "        prop_tokens = tokenizer.encode(prop)\n",
    "\n",
    "        if len(current_tokens) + len(prop_tokens) <= max_tokens:\n",
    "            current_chunk += prop\n",
    "            current_tokens.extend(prop_tokens)\n",
    "        else:\n",
    "            chunks.append((current_chunk, len(current_tokens)))\n",
    "            current_chunk = header + prop\n",
    "            current_tokens = header_tokens.copy() + prop_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append((current_chunk, len(current_tokens)))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def transform_interface(key, value) -> Tuple[str, List[str]]:\n",
    "    header = f\"Interface: {key}\\nSummary: {value['summary']}\\n\\nProperties:\\n\"\n",
    "    properties = []\n",
    "\n",
    "    for prop in value[\"properties\"]:\n",
    "        summary = prop[\"summary\"].strip().replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        prop_text = f\"- {prop['name']}\\n\"\n",
    "        prop_text += f\"   - Type: {prop['type']}\\n\"\n",
    "        prop_text += f\"   - Description: {summary}\\n\"\n",
    "        properties.append(prop_text)\n",
    "\n",
    "    return header, properties\n",
    "\n",
    "\n",
    "def extract_metadata(\n",
    "    key, value, embedding_size=0, chunk_id=0, total_chunks=1\n",
    ") -> Dict[str, Any]:\n",
    "    filename = key\n",
    "    if total_chunks > 1:\n",
    "        filename = key + f\"_chunk_{chunk_id}\"\n",
    "    return {\n",
    "        \"name\": key,\n",
    "        \"summary\": value[\"summary\"],\n",
    "        \"type\": value[\"type\"],\n",
    "        \"namespace\": value[\"namespace\"],\n",
    "        \"assembly\": value[\"assembly\"],\n",
    "        \"type_references\": value[\"type_references\"],\n",
    "        \"filename\": f\"{filename}.txt\",\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"expected_embedding_size\": embedding_size,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load all interfaces from the single JSON file\n",
    "with open(\"scraped_domain_knowledge.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    interfaces = json.load(f)\n",
    "\n",
    "# Transform and save the documents\n",
    "for key, value in interfaces.items():\n",
    "    header, properties = transform_interface(key, value)\n",
    "\n",
    "    # Split the document intelligently if it exceeds the token limit\n",
    "    chunks = split_text_intelligently(properties, header, max_tokens=max_tokens)\n",
    "    total_chunks = len(chunks)\n",
    "    metadata = extract_metadata(key, value)\n",
    "\n",
    "    for chunk_id, (chunk, size) in enumerate(chunks):\n",
    "        chunk_suffix = f\"_chunk_{chunk_id}\" if total_chunks > 1 else \"\"\n",
    "        chunk_key = f\"{key}{chunk_suffix}\"\n",
    "\n",
    "        # Save the text document chunk\n",
    "        with open(f\"documents/{chunk_key}.txt\", \"w\") as f:\n",
    "            f.write(chunk)\n",
    "\n",
    "        # Adjust metadata for chunks\n",
    "        chunk_metadata = extract_metadata(key, value, size, chunk_id, total_chunks)\n",
    "\n",
    "        # Save the metadata for the chunk\n",
    "        with open(f\"documents/{chunk_key}.metadata.json\", \"w\") as f:\n",
    "            json.dump(chunk_metadata, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
