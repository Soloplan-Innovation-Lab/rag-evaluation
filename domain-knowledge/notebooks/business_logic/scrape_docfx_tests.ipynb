{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapes ``https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.*`` in order to get all business objects and their properties.\n",
    "\n",
    "To begin with, the most interesting interfaces (``ITour``, ``IConsignment``, ``ITransportOrder`` and ``ITourStatus``) are scraped. If a property is a complex type (which means another interface, class or enum from us), the scraper will recursively scrape the properties of the complex type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"/workspace/data/business_objects\"\n",
    "TRANSFORMED_DATA_OUTPUT_FOLDER_NAME = \"documents\"\n",
    "SCRAPE_FILE_NAME = \"scraped_domain_knowledge.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "\n",
    "BASE_URL = \"https://docs.soloplan.de/carlo/api/\"\n",
    "\n",
    "DOCS_TO_SCRAPE = [\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITour.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.IConsignment.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITransportOrder.html\",\n",
    "    \"https://docs.soloplan.de/carlo/api/Soloplan.CarLo.Business.ITourStatus.html\",\n",
    "]\n",
    "\n",
    "saved_interfaces = set()\n",
    "\n",
    "\n",
    "def extract_docfx_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract the main content article\n",
    "    article = soup.find(\"article\", class_=\"content wrap\")\n",
    "\n",
    "    # Ensure article is found\n",
    "    if not article:\n",
    "        print(f\"Article not found at {url}.\")\n",
    "        return\n",
    "\n",
    "    # Extract interface details\n",
    "    name = article.find(\"h1\").text.strip()\n",
    "    if name.startswith(\"Interface \"):\n",
    "        name = name[10:]\n",
    "        data_type = \"interface\"\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    if name in saved_interfaces:\n",
    "        print(f\"Skipping {name} as it already exists.\")\n",
    "        return\n",
    "    saved_interfaces.add(name)\n",
    "\n",
    "    # search for <div class=\"markdown level0 summary\">, that is next to the h1 and get the summary text\n",
    "    overall_summary = \"\"\n",
    "    summary_tag = article.find_next(\"div\", class_=\"markdown level0 summary\")\n",
    "    if summary_tag:\n",
    "        summary_p = summary_tag.find(\"p\")\n",
    "        if summary_p:\n",
    "            overall_summary = summary_p.text.strip()\n",
    "\n",
    "    if name.startswith(\"Class \"):\n",
    "        name = name[7:]\n",
    "        data_type = \"class\"\n",
    "    if name.startswith(\"Enum \"):\n",
    "        name = name[6:]\n",
    "        data_type = \"enum\"\n",
    "\n",
    "    # Extract Namespace and Assembly\n",
    "    namespace = \"Namespace not found.\"\n",
    "    assembly = \"Assembly not found.\"\n",
    "    for h6 in article.find_all(\"h6\"):\n",
    "        text = h6.text.strip()\n",
    "        if text.startswith(\"Namespace\"):\n",
    "            namespace = text[11:].strip()\n",
    "        elif text.startswith(\"Assembly\"):\n",
    "            assembly = text[10:].strip()\n",
    "\n",
    "    # check for a h5 element with \"Syntax\" as content\n",
    "    syntax_tag = article.find(\"h5\", string=\"Syntax\")\n",
    "    if syntax_tag:\n",
    "        root_declaration_code = (\n",
    "            syntax_tag.find_next(\"div\", class_=\"codewrapper\").find(\"code\").text.strip()\n",
    "        )\n",
    "    else:\n",
    "        # find the first code block in the article\n",
    "        root_declaration_code = article.find(\"code\").text.strip()\n",
    "\n",
    "    # Extract properties and methods\n",
    "    properties = []\n",
    "    type_references = []\n",
    "    property_headers = article.find_all(\"h4\")\n",
    "    for header in property_headers:\n",
    "        property_name = header.text.strip()\n",
    "        property_summary_tag = header.find_next(\"div\", class_=\"markdown level1 summary\")\n",
    "        if property_summary_tag:\n",
    "            try:\n",
    "                property_summary = property_summary_tag.find(\"p\").text.strip()\n",
    "            except AttributeError:\n",
    "                property_summary = \"Summary not found.\"\n",
    "        else:\n",
    "            property_summary = \"Summary not found.\"\n",
    "\n",
    "        declaration_code_tag = header.find_next(\"div\", class_=\"codewrapper\")\n",
    "        if declaration_code_tag:\n",
    "            declaration_code = declaration_code_tag.find(\"code\").text.strip()\n",
    "        else:\n",
    "            declaration_code = \"Declaration not found.\"\n",
    "\n",
    "        property_value_type_tag = header.find_next(\"h5\", string=\"Property Value\")\n",
    "        if property_value_type_tag:\n",
    "            property_value_type_td = property_value_type_tag.find_next(\"td\")\n",
    "            if property_value_type_td.find(\"a\"):\n",
    "                property_value_type = property_value_type_td.find(\"a\").text.strip()\n",
    "                type_url = BASE_URL + property_value_type_td.find(\"a\")[\"href\"]\n",
    "                # recursively scrape the type if it's not already in the list\n",
    "                if type_url not in DOCS_TO_SCRAPE:\n",
    "                    DOCS_TO_SCRAPE.append(type_url)\n",
    "                    # save the recursive type reference\n",
    "                    type_references.append(property_value_type)\n",
    "            else:\n",
    "                property_value_type = property_value_type_td.text.strip()\n",
    "        else:\n",
    "            property_value_type = \"Property type not found.\"\n",
    "\n",
    "        properties.append(\n",
    "            {\n",
    "                \"name\": property_name,\n",
    "                \"summary\": property_summary,\n",
    "                \"declaration\": declaration_code,\n",
    "                \"type\": property_value_type,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Extract extension methods\n",
    "    extension_methods = []\n",
    "    extension_header = article.find(\"h3\", text=\"Extension Methods\")\n",
    "    if extension_header:\n",
    "        links = extension_header.find_next_siblings(\"div\")\n",
    "        for link in links:\n",
    "            method_name = link.find(\"a\").text.strip()\n",
    "            extension_methods.append(method_name)\n",
    "    else:\n",
    "        extension_methods = [\"Extension methods not found.\"]\n",
    "\n",
    "    return (\n",
    "        name,\n",
    "        {\n",
    "            \"summary\": overall_summary,\n",
    "            \"type\": data_type,\n",
    "            \"namespace\": namespace,\n",
    "            \"assembly\": assembly,\n",
    "            \"declaration\": root_declaration_code,\n",
    "            \"properties\": properties,\n",
    "            \"extension_methods\": extension_methods,\n",
    "            \"type_references\": type_references,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "all_interfaces = {}\n",
    "\n",
    "while DOCS_TO_SCRAPE:\n",
    "    url = DOCS_TO_SCRAPE.pop(0)\n",
    "    result = extract_docfx_data(url)\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    name, item = result\n",
    "    if name in all_interfaces:\n",
    "        continue\n",
    "\n",
    "    all_interfaces[name] = item\n",
    "    print(f\"Processed {name}\")\n",
    "\n",
    "# Save all interfaces to a single JSON file\n",
    "with open(os.path.join(ROOT_FOLDER, SCRAPE_FILE_NAME), \"w\", encoding=\"utf8\") as f:\n",
    "    ordered_interfaces = OrderedDict(sorted(all_interfaces.items()))\n",
    "    json.dump(ordered_interfaces, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embeddings, the JSON structure might not be the best choice, thus these files need to be converted to plain text. Additional metdata will be extracted and added to the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import tiktoken\n",
    "\n",
    "TRANSFORMED_DATA_OUTPUT_FOLDER = os.path.join(\n",
    "    ROOT_FOLDER, TRANSFORMED_DATA_OUTPUT_FOLDER_NAME\n",
    ")\n",
    "\n",
    "# Ensure you have the necessary directory structure\n",
    "if not os.path.exists(TRANSFORMED_DATA_OUTPUT_FOLDER):\n",
    "    os.makedirs(TRANSFORMED_DATA_OUTPUT_FOLDER)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "max_tokens = 8192\n",
    "\n",
    "\n",
    "def split_text_intelligently(\n",
    "    properties: List[str], header: str, max_tokens=8192, max_bytes=32766\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Split the text intelligently based on the token limit. Since properties of interfaces are mostly used in this dataset, 'intelligently' means that we split the properties in a way that they are not split across multiple chunks.\n",
    "\n",
    "    Args:\n",
    "        properties (List[str]): List of property strings.\n",
    "        header (str): File header string.\n",
    "        max_tokens (int): Maximum number of token that the embedding model can handle.\n",
    "        max_bytes (int): Maximum number of bytes allowed in a chunk (based on Azure AI Search limits).\n",
    "    \"\"\"\n",
    "    header_tokens = tokenizer.encode(header)\n",
    "    chunks = []\n",
    "\n",
    "    current_chunk = header\n",
    "    current_tokens = header_tokens.copy()\n",
    "\n",
    "    for prop in properties:\n",
    "        prop_tokens = tokenizer.encode(prop)\n",
    "\n",
    "        if len(current_tokens) + len(prop_tokens) <= max_tokens:\n",
    "            current_chunk += prop\n",
    "            current_tokens.extend(prop_tokens)\n",
    "        else:\n",
    "            chunks.append((current_chunk, len(current_tokens)))\n",
    "            current_chunk = header + prop\n",
    "            current_tokens = header_tokens.copy() + prop_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append((current_chunk, len(current_tokens)))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_propertyid_and_caption(content: str) -> Tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Extract the property ID and caption from the [SoloProperty()] attribute in the content.\n",
    "\n",
    "    Args:\n",
    "        content (str): The content to extract the property ID and caption from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, str]: The property ID and caption.\n",
    "    \"\"\"\n",
    "    # Find the SoloProperty attribute\n",
    "    match = re.search(r\"\\[SoloProperty\\((.*?)\\)\\]\", content)\n",
    "    if not match:\n",
    "        return None, None  # or raise an exception\n",
    "\n",
    "    # Extract the arguments as a single string\n",
    "    arguments_str = match.group(1)\n",
    "\n",
    "    # Split the arguments considering potential commas within strings\n",
    "    arguments = []\n",
    "    temp_arg = \"\"\n",
    "    in_quotes = False\n",
    "    for char in arguments_str:\n",
    "        if char == '\"' and not in_quotes:\n",
    "            in_quotes = True\n",
    "        elif char == '\"' and in_quotes:\n",
    "            in_quotes = False\n",
    "        if char == \",\" and not in_quotes:\n",
    "            arguments.append(temp_arg.strip())\n",
    "            temp_arg = \"\"\n",
    "        else:\n",
    "            temp_arg += char\n",
    "    arguments.append(temp_arg.strip())  # Add the last argument\n",
    "\n",
    "    # Extract the first and third argument, ensuring to strip quotes\n",
    "    property_id = int(arguments[0])\n",
    "    caption = arguments[2].strip('\"')\n",
    "\n",
    "    return property_id, caption\n",
    "\n",
    "\n",
    "def transform_interface(key: str, value: object) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Transform the interface into a text document.\n",
    "\n",
    "    Args:\n",
    "        key (str): The interface name.\n",
    "        value (object): The interface object.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: The header and properties as strings.\n",
    "    \"\"\"\n",
    "\n",
    "    ipropertyid, icaption = extract_propertyid_and_caption(value[\"declaration\"])\n",
    "\n",
    "    header = f\"Interface: {key}\\n\"\n",
    "    header += f\"Summary: {value['summary']}\\n\"\n",
    "    header += f\"PropertyId: {ipropertyid}\\n\"\n",
    "    header += f\"Caption: {icaption}\\n\\nProperties:\"\n",
    "    properties = []\n",
    "\n",
    "    for prop in value[\"properties\"]:\n",
    "        ppropertyid, pcaption = extract_propertyid_and_caption(prop[\"declaration\"])\n",
    "\n",
    "        summary = prop[\"summary\"].strip().replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        prop_text = f\"- {prop['name']}\\n\"\n",
    "        prop_text += f\"   - Type: {prop['type']}\\n\"\n",
    "        prop_text += f\"   - Description: {summary}\\n\"\n",
    "        prop_text += f\"   - PropertyId: {ppropertyid}\\n\"\n",
    "        prop_text += f\"   - Caption: {pcaption}\\n\\n\"\n",
    "        properties.append(prop_text)\n",
    "\n",
    "    return header, properties\n",
    "\n",
    "\n",
    "def extract_metadata(\n",
    "    key, value, embedding_size=0, chunk_id=0, total_chunks=1\n",
    ") -> Dict[str, Any]:\n",
    "    filename = key\n",
    "    if total_chunks > 1:\n",
    "        filename = key + f\"_chunk_{chunk_id}\"\n",
    "    return {\n",
    "        \"name\": key,\n",
    "        \"summary\": value[\"summary\"],\n",
    "        \"type\": value[\"type\"],\n",
    "        \"namespace\": value[\"namespace\"],\n",
    "        \"assembly\": value[\"assembly\"],\n",
    "        \"type_references\": value[\"type_references\"],\n",
    "        \"filename\": f\"{filename}.txt\",\n",
    "        \"chunk_id\": chunk_id,\n",
    "        \"total_chunks\": total_chunks,\n",
    "    }\n",
    "\n",
    "\n",
    "# Load all interfaces from the single JSON file\n",
    "with open(os.path.join(ROOT_FOLDER, SCRAPE_FILE_NAME), \"r\", encoding=\"utf8\") as f:\n",
    "    interfaces: Dict[str, object] = json.load(f)\n",
    "\n",
    "# Transform and save the documents\n",
    "for key, value in interfaces.items():\n",
    "    header, properties = transform_interface(key, value)\n",
    "\n",
    "    # Split the document intelligently if it exceeds the token limit\n",
    "    chunks = split_text_intelligently(properties, header, max_tokens=max_tokens)\n",
    "    total_chunks = len(chunks)\n",
    "    metadata = extract_metadata(key, value)\n",
    "\n",
    "    for chunk_id, (chunk, size) in enumerate(chunks):\n",
    "        chunk_suffix = f\"_chunk_{chunk_id}\" if total_chunks > 1 else \"\"\n",
    "        chunk_key = f\"{key}{chunk_suffix}\"\n",
    "\n",
    "        # Save the text document chunk\n",
    "        with open(\n",
    "            os.path.join(TRANSFORMED_DATA_OUTPUT_FOLDER, f\"{chunk_key}.txt\"), \"w\"\n",
    "        ) as f:\n",
    "            f.write(chunk)\n",
    "\n",
    "        # Adjust metadata for chunks\n",
    "        chunk_metadata = extract_metadata(key, value, size, chunk_id, total_chunks)\n",
    "\n",
    "        # Save the metadata for the chunk\n",
    "        with open(\n",
    "            os.path.join(TRANSFORMED_DATA_OUTPUT_FOLDER, f\"{chunk_key}.metadata.json\"),\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            json.dump(chunk_metadata, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
