{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit domain knowledge to Azure AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "from internal_shared.models.ai import EMBEDDING_3_LARGE\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=EMBEDDING_3_LARGE.api_key,\n",
    "    api_version=EMBEDDING_3_LARGE.api_version,\n",
    "    azure_endpoint=EMBEDDING_3_LARGE.endpoint,\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"AZURE_AI_SEARCH_INDEX\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_AI_SEARCH_API_KEY\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "\n",
    "def embed_texts(texts):\n",
    "    # Create embeddings for a batch of texts\n",
    "    embedding_results = client.embeddings.create(\n",
    "        input=texts, model=EMBEDDING_3_LARGE.model_name\n",
    "    )\n",
    "    return [result.embedding for result in embedding_results.data]\n",
    "\n",
    "\n",
    "def create_search_document(content, embedding, metadata: Dict[str, Any]):\n",
    "    # Create a unique id for each document\n",
    "    document_id = str(uuid.uuid4())\n",
    "\n",
    "    # Combine content and metadata\n",
    "    document = {\n",
    "        \"id\": document_id,\n",
    "        \"content\": content,\n",
    "        \"embedding\": embedding,\n",
    "        \"name\": metadata.get(\"name\", \"\"),\n",
    "        \"summary\": metadata.get(\"summary\", \"\"),\n",
    "        \"type\": metadata.get(\"type\", \"\"),\n",
    "        \"namespace\": metadata.get(\"namespace\", \"\"),\n",
    "        \"assembly\": metadata.get(\"assembly\", \"\"),\n",
    "        \"type_references\": metadata.get(\"type_references\", []),\n",
    "        \"filename\": metadata.get(\"filename\", \"\"),\n",
    "        \"chunk_id\": metadata.get(\"chunk_id\", 0),\n",
    "        \"total_chunks\": metadata.get(\"total_chunks\", 1),\n",
    "        \"expected_embedding_size\": metadata.get(\"expected_embedding_size\", 0),\n",
    "    }\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "# Collect all documents\n",
    "documents = []\n",
    "contents = []\n",
    "metadata_list = []\n",
    "for filename in os.listdir(\"documents\"):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        interface_name = filename[:-4]\n",
    "\n",
    "        # Read the text content\n",
    "        with open(f\"documents/{interface_name}.txt\", \"r\") as f:\n",
    "            content = f.read()\n",
    "            contents.append(content)\n",
    "\n",
    "        # Read the metadata\n",
    "        with open(f\"documents/{interface_name}.metadata.json\", \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "            metadata_list.append(metadata)\n",
    "\n",
    "# Create embeddings in batches\n",
    "batch_size = 1000  # we can access around 120k tokens per minute\n",
    "embedding_batches = (len(contents) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in range(0, len(contents), batch_size):\n",
    "    batch_contents = contents[i : i + batch_size]\n",
    "    batch_embeddings = embed_texts(batch_contents)\n",
    "\n",
    "    for j in range(len(batch_contents)):\n",
    "        document = create_search_document(\n",
    "            batch_contents[j], batch_embeddings[j], metadata_list[i + j]\n",
    "        )\n",
    "        documents.append(document)\n",
    "\n",
    "    # before next iteration, wait time to avoid rate limiting\n",
    "    if (i // embedding_batches + 1) < embedding_batches:\n",
    "        time.sleep(30)\n",
    "\n",
    "print(f\"Embedded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload documents to Azure AI Search; supports uploading in batches up to 1000 documents\n",
    "UPLOAD_BATCH_SIZE = 1000\n",
    "upload_batches = (len(documents) + UPLOAD_BATCH_SIZE - 1) // UPLOAD_BATCH_SIZE\n",
    "\n",
    "for i in range(0, len(documents), UPLOAD_BATCH_SIZE):\n",
    "    batch = documents[i : i + UPLOAD_BATCH_SIZE]\n",
    "    results = search_client.upload_documents(documents=batch)\n",
    "    if all(result.succeeded for result in results):\n",
    "        print(f\"Uploaded batch {i // UPLOAD_BATCH_SIZE + 1} successfully.\")\n",
    "    else:\n",
    "        # check, which results failed\n",
    "        for result in results:\n",
    "            if not result.succeeded:\n",
    "                print(\n",
    "                    f\"Failed to upload document with ID {result.key}. Error: {result.error_message}\"\n",
    "                )\n",
    "\n",
    "    # don't hit rate limits\n",
    "    if (i // UPLOAD_BATCH_SIZE + 1) < upload_batches:\n",
    "        time.sleep(30)\n",
    "\n",
    "print(f\"All {len(results)} documents uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_results = search_client.delete_documents(documents)\n",
    "if all(result.succeeded for result in delete_results):\n",
    "    print(f\"Deleted batch {i // UPLOAD_BATCH_SIZE + 1} successfully.\")\n",
    "else:\n",
    "    for result in delete_results:\n",
    "        if not result.succeeded:\n",
    "            print(f\"Failed to upload document with ID {result.key}. Error: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit formula functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "from internal_shared.models.documents import DevExpressFunction\n",
    "from internal_shared.models.ai import EMBEDDING_3_LARGE\n",
    "\n",
    "_embed_client = AzureOpenAI(\n",
    "    api_key=EMBEDDING_3_LARGE.api_key,\n",
    "    api_version=EMBEDDING_3_LARGE.api_version,\n",
    "    azure_endpoint=EMBEDDING_3_LARGE.endpoint,\n",
    ")\n",
    "\n",
    "_search_client = SearchClient(\n",
    "    endpoint=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "    index_name=\"mergedfunctionindex_v2\",\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_AI_SEARCH_API_KEY\")),\n",
    ")\n",
    "\n",
    "def load_json(file_path: str) -> List[DevExpressFunction]:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return [DevExpressFunction(**item) for item in json.load(file)]\n",
    "\n",
    "def convert_to_search_document(embedding: List[float], data: DevExpressFunction) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": data.description,\n",
    "        \"embedding\": embedding,\n",
    "        \"name\": data.name,\n",
    "        \"example\": data.example,\n",
    "        \"category\": data.category,\n",
    "        \"source\": data.source,\n",
    "        \"keywords\": data.keywords or [],\n",
    "    }\n",
    "\n",
    "def embed_all_text(text: List[str]) -> List[List[float]]:\n",
    "    embeddings = _embed_client.embeddings.create(\n",
    "        input=text, model=EMBEDDING_3_LARGE.model_name\n",
    "    )\n",
    "    return [result.embedding for result in embeddings.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read custom and native function and convert them to search documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = load_json(\"/workspace/data/functions/data.custom.json\")\n",
    "native_documents = load_json(\"/workspace/data/functions/data.json\")\n",
    "documents.extend(native_documents)\n",
    "descriptions = [document.description for document in documents]\n",
    "\n",
    "len(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed and convert the code to search documents in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "search_documents = []\n",
    "embedding_batch_size = 1000\n",
    "total_embedding_batches = (len(descriptions) + embedding_batch_size - 1) // embedding_batch_size\n",
    "\n",
    "for i in range(0, len(descriptions), embedding_batch_size):\n",
    "    batch_descriptions = descriptions[i : i + embedding_batch_size]\n",
    "    batch_embeddings = embed_all_text(batch_descriptions)\n",
    "\n",
    "    for j in range(len(batch_descriptions)):\n",
    "        document = convert_to_search_document(batch_embeddings[j], documents[i + j])\n",
    "        search_documents.append(document)\n",
    "\n",
    "    # before next iteration, wait time to avoid rate limiting\n",
    "    if (i // embedding_batch_size + 1) < total_embedding_batches:\n",
    "        time.sleep(30)\n",
    "\n",
    "len(search_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the created search documents in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded batch 1 successfully.\n",
      "All 208 documents uploaded.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "upload_batch_size = 1000\n",
    "total_upload_batches = (\n",
    "    len(search_documents) + upload_batch_size - 1\n",
    ") // upload_batch_size\n",
    "\n",
    "for i in range(0, len(search_documents), upload_batch_size):\n",
    "    batch = search_documents[i : i + upload_batch_size]\n",
    "    results = _search_client.upload_documents(documents=batch)\n",
    "    if all(result.succeeded for result in results):\n",
    "        print(f\"Uploaded batch {i // upload_batch_size + 1} successfully.\")\n",
    "    else:\n",
    "        for result in results:\n",
    "            if not result.succeeded:\n",
    "                print(\n",
    "                    f\"Failed to upload document with ID {result.key}. Error: {result.error_message}\"\n",
    "                )\n",
    "\n",
    "    # ensure we don't hit the rate limit\n",
    "    if (i // upload_batch_size + 1) < total_upload_batches:\n",
    "        time.sleep(30)\n",
    "\n",
    "print(f\"All {len(results)} documents uploaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
